name: mosaic-gpt-1b-gpus-8
image: mosaicml/composer:0.13.5 # This image includes Composer
# image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04 # This image does not include Composer
# You can find other images ready to use on the Mosaic platform here: https://hub.docker.com/u/mosaicml
gpu_num: 8
gpu_type: a100_40gb

integrations:
- integration_type: git_repo
  git_repo: ygong1/llm-foundry
  git_branch: main # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[gpu]
  ssh_clone: false # Should be true if using a private repo

command: |
  cd llm-foundry/scripts
  python data_prep/convert_dataset_hf.py --dataset c4 --data_subset en --out_root ./my-copy-c4 --splits train_small val \
    --concat_tokens 2048 --tokenizer gpt2 --eos_text '<|endoftext|>'
  composer train/train.py train/yamls/pretrain/mpt-1b.yaml \
    train_loader.dataset.split=train_small \
    max_duration=100ba \
    eval_interval=0

# The below is injected as a YAML file: /mnt/config/parameters.yaml
# but is not used in this example.
parameters: {}
